{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM Algorithm\n",
    "\n",
    "## Maximum Likelihood Estimation\n",
    "\n",
    "Everyone starts with theory, I'm going to start with simple example of flipping coins. It is rare that we actually want to know something about flipping coins, but we *do* care about distributions in disease in populations and many other things which are structured the same way (bad wording).\n",
    "\n",
    "Start with a set of coins with unknown bias $\\beta$. Want to estimate bias.\n",
    "\n",
    "\n",
    "Easy. Flip them, and compute bias as $\\frac{\\text{number of heads}}{\\text{number of flips}}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias: 0.600\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import random\n",
    "\n",
    "TRIALS = 100\n",
    "bias = 0.612\n",
    "flips = [random() < bias for _ in range(TRIALS)]\n",
    "heads_count = sum(flips)\n",
    "\n",
    "print('bias: {:.3f}'.format(heads_count / TRIALS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**something about convergence rate**\n",
    "\n",
    "I won't motivate this yet, but now suppose we have 10 coins, where coin $i$ has some bias $\\beta_i$. We can compute the bias of each set as we did above. I'll make it easier by putting the computation in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_coin_bias_first_attempt(bias, TRIALS):\n",
    "    flips = [random() < bias for _ in range(TRIALS)]\n",
    "    heads_count = sum(flips)\n",
    "    return heads_count / TRIALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6126"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_coin_bias_first_attempt(.612, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is not a very efficient function. Lets use NumPy to speed it up. NumPy allows us to use conditional operators on an entire array. If I have an array `x`, then `x < .3` will create a new boolean array of the same length. Each element will be true if the conditional is valid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True  True False False  True False  True]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([4, 1, 2, 6, 4, 3, 8, 1])\n",
    "print(x <= 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can count the number of elements that are true by using Numpy's `sum` routine. Do not use the built in routine sum - that uses pure Python to iterate over a list and it is very slow compared to NumPy's version, which is pure C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(sum(x <=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_coin_bias(bias, TRIALS):\n",
    "    rand_nums = random(TRIALS)\n",
    "    return np.sum(rand_nums <= bias) / TRIALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.612706\n"
     ]
    }
   ],
   "source": [
    "print(compute_coin_bias(.612, 1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `%timeit` magic to compare the speeds of these functions. Your machine may be different, but I get approximately a 13x speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 13 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit compute_coin_bias_first_attempt(.612, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 3: 949 Âµs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit compute_coin_bias(.612, 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to our problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set  1 has computed bias  0.1200 for the actual bias 0.1262\n",
      "set  2 has computed bias  0.5400 for the actual bias 0.5288\n",
      "set  3 has computed bias  0.7900 for the actual bias 0.7732\n",
      "set  4 has computed bias  0.9900 for the actual bias 0.9892\n",
      "set  5 has computed bias  0.7000 for the actual bias 0.6991\n",
      "set  6 has computed bias  0.2400 for the actual bias 0.2447\n",
      "set  7 has computed bias  0.9000 for the actual bias 0.8506\n",
      "set  8 has computed bias  0.4400 for the actual bias 0.4897\n",
      "set  9 has computed bias  0.7000 for the actual bias 0.6533\n",
      "set 10 has computed bias  0.5000 for the actual bias 0.4674\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    # create a random bias for this set of coins\n",
    "    bias = random()\n",
    "    beta = compute_coin_bias(bias, 100)\n",
    "    \n",
    "    print('set {:2d} has computed bias  {:.4f} '\n",
    "          'for the actual bias {:.4f}'.format(i+1, beta, bias))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is any number of problems for which this applies. We may want to count cancer incidence in different age groups, sexes, or geographic locations. We may be doing quality control on samples drawn from different runs at a factory, such as batches of pharmaceutical drugs. It doesn't matter. \n",
    "\n",
    "So far this is very straightforward. Let me introduce some terminology and concepts. First, the computation $\\frac{\\text{number of heads}}{\\text{number of flips}}$ is based on an assumption of a **uniform, stationary process** of coin flip results. I.e., we are assuming that the proportion of heads to tails is equal at the start of the trial as at the end, and that the bias of a coin does not vary over time. If the bias reduced by 0.01 per second obviously the computation would not produce the correct result. \n",
    "\n",
    "Next, the coin bias in this experiment is called a **parameter**, and our attempt to estimate the bias is known as **parameter estimation**.\n",
    "\n",
    "Combining the ideas of the last two paragraphs we come up with this. We are trying to estimate the parameters of a statistical model for a process. That is, I make a statistical model of our process, the coin flip. I assume that the flips are uniformly distributed and stationary. I don't know this is true, I am assuming it. Given that assumption, I inspect the data of the coin flips and *estimate* the parameter *bias*. \n",
    "\n",
    "What we have done is known as **maximum likelihood estimation**, or MLE. MLE estimates the parameters of a statistical model. The literature uses $\\theta$ to represent the set of parameters (there is usually more than one) which we are estimating, and the MLE equation is written as\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\hat{\\theta} &= \\underset{\\theta}{\\operatorname{argmax}}\\,\\bigg[ P(x_{1..n}|\\theta)\\bigg] \\\\\n",
    "&= \\underset{\\theta}{\\operatorname{argmax}}\\,\\bigg[\\prod\\limits_{i=1}^N P(x_i|\\theta)\\bigg]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining the MLE Equations\n",
    "\n",
    "This requires some explanation. $x_i$ is the i$^{th}$ data point. For our coin experiment it is one coin flip, so $x_1$  would be the result of the first coin flip. $x_{1..n}$ is the notation for the set of all $N$ flips. Next, $P(x_{1..n}|\\theta)$ is the notation for the probability for the set of coin flips $x_{1..n}$ *given* the parameters with the value $\\theta$. The bar in the equation ($|$) means *given*, and is pronounced as the English word *given*. \n",
    "\n",
    "Let's be specific to ensure you understand this. For our coin flip experiments $\\theta$ is a single parameter, the bias of the coin, and $x_{1..n}$ is the set of flips in one experiment. Then, $P(x_{1..n}|\\theta=.6)$ would be the probability that the set of coin flips would happen *if* the coin bias was 0.6. We are not asserting that the bias *is* 0.6, we are merely computing the probability of the flips occurring *if and only if* the bias was 0.6.\n",
    "\n",
    "We are not interested in the probability of the coin flips given some arbitrary bias, we want to *estimate* the bias that gives us the highest probability for the set of coin flips. Naively, you can think of us wanting to compute:\n",
    "\n",
    "```python\n",
    "max_prob = 0\n",
    "for bias in np.arange(0, 1, step=1.e-8):\n",
    "    prob_bias = compute_probability(flips, bias)\n",
    "    max_prob = max(max_prob, prob_bias)\n",
    "```\n",
    "This would compute the bias to within 8 digits of precision, but it is a terribly slow and inefficient algorithm compared to our simple computation of the number of heads divided by the number of flips. But most interesting problems do not have such a easy way to compute the results. \n",
    "\n",
    "The notation $\\underset{\\theta}{\\operatorname{argmax}}\\,\\bigg[ P(x_{1..n}|\\theta)\\bigg]$ expresses the idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> compute the value for the parameters ($\\theta$) using some unspecified algorithm that has the highest probability (maximum likelihood) of being true for the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a *maximum likelihood estimator* because it is estimating the paramaters $\\theta$ which has the maximum likelihood of being true given the data set and assumed statistical model. \n",
    "\n",
    "That is pretty easy. What about $\\underset{\\theta}{\\operatorname{argmax}}\\,\\bigg[\\prod\\limits_{i=1}^N P(x_i|\\theta)\\bigg]$? This is based on the way to compute the probability for the set of $N$ independent trials is the product of the probabilities for each trial:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "P(x_{1..n}|\\theta) &= P(x_1|\\theta)\\, \\times\\, P(x_2|\\theta)\\, \\times\\, P(x_2|\\theta)\\, \\times\\, ...\\, \\times\\, P(x_n|\\theta) \\\\\n",
    "&= \\prod\\limits_{i=1}^N P(x_i|\\theta)\n",
    "\\end{aligned}$$\n",
    "\n",
    "For our simple coin flipping example we can compute $\\underset{\\theta}{\\operatorname{argmax}}\\,\\bigg[ P(x_{1..n}|\\theta)\\bigg]$ with \n",
    "```python\n",
    "def compute_coin_bias(bias, TRIALS):\n",
    "    rands = random(TRIALS)\n",
    "    return np.sum(rands <= bias) / TRIALS\n",
    "```\n",
    "\n",
    "but most problems are not so easy. There is a lot I could say about solving this problem in general, but the EM Algorithm that we will be learning side-steps this problem nicely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-likelihood\n",
    "\n",
    "I will add one concept to this before I continue. Some of the difficulties in computing the MLE can be reduced by introducing the concept of the **log-likelihood**, which is just the logarithm of the likelihood function. Why would we do this? \n",
    "\n",
    "To find the maximum value for some function $f(x)$ we take the derivative of it and set it to zero, like this: $f'(x) = 0$. This requires that the function be differentiable, which is often either very difficult or impossible to do. \n",
    "\n",
    "In our case the function is a probability distribution. The logarithm is a *monotonically increasing* function. In other words, if we take the logarithm of a likelihood function the logarithm will reach its maximum value at the same points that the likelihood function does. This is true both for the local maxima and global maxima. \n",
    "\n",
    "Let's see this in code and a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEACAYAAABRQBpkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQCmlFJoMlFJKAf8fXKXirIZVuiAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x540d7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import randn\n",
    "\n",
    "def norm(x, x0, sigma):\n",
    "    return np.exp(-0.5 * (x - x0) ** 2 / sigma ** 2)\n",
    "\n",
    "# compute some arbitrary multimodal distribution\n",
    "x = np.linspace(0, 1, 200)\n",
    "likelihood = norm(x, .2, .05) + .35*norm(x, .55, .08)\n",
    "likelihood /=  np.sum(likelihood) # normalize it\n",
    "\n",
    "log_likelihood = np.log(likelihood)\n",
    "# make it positive and normalized.\n",
    "log_likelihood += -np.min(log_likelihood)\n",
    "log_likelihood /= np.sum(log_likelihood)\n",
    "\n",
    "# plot them\n",
    "plt.plot(x, likelihood, label='likelihood')\n",
    "plt.plot(x, log_likelihood, label='log likelihood')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can show that the maximum value of each is at the same index with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(likelihood))\n",
    "print(np.argmax(log_likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed they are the same. I haven't proven that this relationship is valid, but it is. I don't want to get bogged down in a proof. See any statistics or machine learning textbook if you can't work it out yourself.\n",
    "\n",
    "If you see log-likelihood being used anywhere just think back to this chart. They have almost certainly introduced the logarithm not for any reason related to the problem, but just to make the computations easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Incomplete Data\n",
    "\n",
    "All of the work above is to prepare us for this, the actual problem. It is easy to compute the bias of 10 coins when we know the flips for each coin. What if we  lost or don't have that association? In other words, we have a sequence of heads and tails recorded, but do not know which coin produced the flips?\n",
    "\n",
    "This is not a silly question, this is a very common situation in practice. For example **some example here**\n",
    "\n",
    "Let's look as a plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10.   4.   4.  10.  10.   4.  10.  10.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEZCAYAAAB7HPUdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x72af9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_trials, num_flips = 8, 20\n",
    "data = np.zeros(num_trials)\n",
    "bias1, bias2 = .7, .3\n",
    "theta = [bias1, bias2]\n",
    "\n",
    "def run_trials(data, theta):\n",
    "    num_trials, = data.shape\n",
    "    flips = random(num_flips)\n",
    "    for i in range(num_trials):\n",
    "        if random() < .5:\n",
    "            data[i] =  np.sum(flips < theta[0])\n",
    "        else:\n",
    "            data[i] = np.sum(flips < theta[1])\n",
    "\n",
    "run_trials(data, theta)\n",
    "            \n",
    "\n",
    "plt.plot(data)\n",
    "plt.title('# of heads per trial')\n",
    "plt.xlabel('trial')\n",
    "plt.ylabel('# heads')\n",
    "plt.ylim(0, 20);\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspection of the code we see that one coin had a bias of $\\beta=0.57$ and the second had a bias of $\\beta=0.43$. At first glance this problem seems intractable. For example, probability allows for a coin with bias $\\beta=.99$ to flip tails 10 times in a row, so we cannot assume that any given trial will have a high number of heads reflects a large bias ($\\beta >> 0.5$). If in the code I made the bias for one coin much higher than the other, such as `bias_1 = 0.97`, `bias_2 = 0.007`, we might be able to solve this problem by inspection just by assuming that trials that are all heads are due to coin one and trials with almost all tails are due to coin two. But the differences in data are not generally so stark.\n",
    "\n",
    "So, in general the problem seems intractable. But, is it? Let's just take a naive approach and see where it gets us.\n",
    "\n",
    "We need to start somewhere, so lets start by assuming some bias for each coin. We have no reason to assume any value, so I will choose a value at random: 0.75 for the first coin, and 0.3 for the second coin. This is *entirely* arbitrary - I did not inspect the data and somehow infer these numbers. We need to start somewhere.\n",
    "\n",
    "Let's get this into standard notation. First, we say that\n",
    "\n",
    "$$\\begin{aligned}\\beta_1 &= 0.6\\\\\\beta_2 &= 0.3\\end{aligned}$$\n",
    "\n",
    "where $\\beta_i$ is the bias for the i$^{th}$ coin. Now we collect them into $\\theta$ for notational convenience:\n",
    "\n",
    "$$\\theta = (\\beta_1, \\beta_2) = (0.6, 0.3)$$\n",
    "\n",
    "In code we might express this as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.6,  0.3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = np.array((0.6, 0.3))\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are very unlikely to be close to accurate, but let's press on. We can now inspect each trial and compute if it is more likely to have been caused by the first coin or second coin based on these biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coin 1 is the closest match for bias 0.5\n",
      "coin 2 is the closest match for bias 0.2\n",
      "coin 2 is the closest match for bias 0.2\n",
      "coin 1 is the closest match for bias 0.5\n",
      "coin 1 is the closest match for bias 0.5\n",
      "coin 2 is the closest match for bias 0.2\n",
      "coin 1 is the closest match for bias 0.5\n",
      "coin 1 is the closest match for bias 0.5\n"
     ]
    }
   ],
   "source": [
    "def compute_assignment(biases, theta):\n",
    "    num_trials = len(biases) \n",
    "    assignment = np.zeros(num_trials, dtype=int)\n",
    "    for i, bias in enumerate(biases):\n",
    "        assignment[i] = np.abs(theta - bias).argmin()        \n",
    "    return assignment\n",
    "\n",
    "bias = data / num_flips\n",
    "assignment = compute_assignment(bias, theta)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    print('coin {} is the closest match for bias {}'\n",
    "          .format(assignment[i]+1, bias[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the assignments, we have an outline for an algorithm. We could start with an initial estimate for $\\theta$ and compute which coin is most likely \n",
    "\n",
    "If we know there is a probability $p$ for an event, and we observe $d$ occurances of that event over $N$ trials, the probability of that occuring is given by\n",
    "\n",
    "$$P(\\text{data} \\, \\lvert\\, p) = \\begin{pmatrix}N\\\\d\\end{pmatrix} p^d (1-p)^{N-d}$$\n",
    "\n",
    "We will turn that into code with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "from scipy.misc import comb\n",
    "\n",
    "def coin_likelihood(N, d, bias):\n",
    "    return comb(N, d, exact=True) * bias**d * (1-bias)**(N-d)\n",
    "\n",
    "def coin_log_likelihood(N, d, bias):\n",
    "    return np.log(comb(N, d, exact=True) * bias**d * (1-bias)**(N-d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10291934520000003\n",
      "0.24609375\n",
      "-2.27380965381\n",
      "-1.40204271809\n"
     ]
    }
   ],
   "source": [
    "print(coin_likelihood(N=10, d=5, bias=.7))\n",
    "print(coin_likelihood(N=10, d=5, bias=.5))\n",
    "print(coin_log_likelihood(N=10, d=5, bias=.7))\n",
    "print(coin_log_likelihood(N=10, d=5, bias=.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url('http://fonts.googleapis.com/css?family=Source+Code+Pro');\n",
       "@import url('http://fonts.googleapis.com/css?family=Vollkorn');\n",
       "@import url('http://fonts.googleapis.com/css?family=Arimo');\n",
       "@import url('http://fonts.googleapis.com/css?family=Fira_sans');\n",
       "\n",
       "    div.cell{\n",
       "        width: 900px;\n",
       "        margin-left: 0% !important;\n",
       "        margin-right: auto;\n",
       "    }\n",
       "    div.text_cell code {\n",
       "        background: transparent;\n",
       "        color: #000000;\n",
       "        font-weight: 600;\n",
       "        font-size: 11pt;\n",
       "        font-style: bold;\n",
       "        font-family:  'Source Code Pro', Consolas, monocco, monospace;\n",
       "   }\n",
       "    h1 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "\t}\n",
       "\t\n",
       "    div.input_area {\n",
       "        background: #F6F6F9;\n",
       "        border: 1px solid #586e75;\n",
       "    }\n",
       "\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 30pt;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h2 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "        text-align: left;\n",
       "    }\n",
       "    .text_cell_render h2 {\n",
       "        font-weight: 200;\n",
       "        font-size: 16pt;\n",
       "        font-style: italic;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1.5em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h3 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h3 {\n",
       "        font-weight: 200;\n",
       "        font-size: 14pt;\n",
       "        line-height: 100%;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 2em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    }\n",
       "    h4 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h4 {\n",
       "        font-weight: 100;\n",
       "        font-size: 14pt;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    h5 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 200;\n",
       "        font-style: normal;\n",
       "        color: #1d3b84;\n",
       "        font-size: 16pt;\n",
       "        margin-bottom: 0em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    div.text_cell_render{\n",
       "        font-family: 'Fira sans', verdana,arial,sans-serif;\n",
       "        line-height: 125%;\n",
       "        font-size: 115%;\n",
       "        text-align:justify;\n",
       "        text-justify:inter-word;\n",
       "    }\n",
       "    div.output_subarea.output_text.output_pyout {\n",
       "        overflow-x: auto;\n",
       "        overflow-y: scroll;\n",
       "        max-height: 50000px;\n",
       "    }\n",
       "    div.output_subarea.output_stream.output_stdout.output_text {\n",
       "        overflow-x: auto;\n",
       "        overflow-y: scroll;\n",
       "        max-height: 50000px;\n",
       "    }\n",
       "    div.output_wrapper{\n",
       "        margin-top:0.2em;\n",
       "        margin-bottom:0.2em;\n",
       "}\n",
       "\n",
       "    code{\n",
       "      font-size: 70%;\n",
       "    }\n",
       "    .rendered_html code{\n",
       "    background-color: transparent;\n",
       "    }\n",
       "    ul{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li li{\n",
       "        padding-left: 0.2em; \n",
       "        margin-bottom: 0.2em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    ol{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ol li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    a:link{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:visited{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:hover{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:focus{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:active{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    .rendered_html :link {\n",
       "       text-decoration: underline; \n",
       "    }\n",
       "    .rendered_html :hover {\n",
       "       text-decoration: none; \n",
       "    }\n",
       "    .rendered_html :visited {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :focus {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :active {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "    } \n",
       "    hr {\n",
       "      color: #f3f3f3;\n",
       "      background-color: #f3f3f3;\n",
       "      height: 1px;\n",
       "    }\n",
       "    blockquote{\n",
       "      display:block;\n",
       "      background: #fcfcfc;\n",
       "      border-left: 5px solid #c76c0c;\n",
       "      font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "      width:680px;\n",
       "      padding: 10px 10px 10px 10px;\n",
       "      text-align:justify;\n",
       "      text-justify:inter-word;\n",
       "      }\n",
       "      blockquote p {\n",
       "        margin-bottom: 0;\n",
       "        line-height: 125%;\n",
       "        font-size: 100%;\n",
       "      }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    scale:100,\n",
       "                        availableFonts: [],\n",
       "                        preferredFont:null,\n",
       "                        webFont: \"TeX\",\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# style the notebook\n",
    "from IPython.core.display import HTML\n",
    "import urllib.request\n",
    "response = urllib.request.urlopen('http://bit.ly/1LC7EI7')\n",
    "HTML(response.read().decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
